{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4feb7621",
   "metadata": {},
   "source": [
    "<h2>Vanishing and Exploding Gradient Problem</h2>\n",
    "\n",
    "<h3>What is it?</h3>\n",
    "\n",
    "<b>English:</b>\n",
    "<p>\n",
    "In deep neural networks, during backpropagation, gradients can become very small (vanish) or very large (explode).  \n",
    "This causes poor training or model instability.\n",
    "</p>\n",
    "\n",
    "<b>বাংলা:</b>\n",
    "<p>\n",
    "ডিপ নিউরাল নেটওয়ার্কে ব্যাকপ্রোপাগেশনের সময় গ্র্যাডিয়েন্ট (gradient) অনেক ছোট হয়ে যেতে পারে (vanishing) অথবা অনেক বড় হয়ে যেতে পারে (exploding)।  \n",
    "এর ফলে মডেল শেখে না বা instable হয়ে পড়ে।\n",
    "</p>\n",
    "\n",
    "<h3>Why it happens:</h3>\n",
    "<ul>\n",
    "  <li>Too many layers</li>\n",
    "  <li>Improper weight initialization</li>\n",
    "  <li>Non-linear activation functions (like sigmoid, tanh)</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Symptoms:</h3>\n",
    "<ul>\n",
    "  <li>Loss stuck at one value</li>\n",
    "  <li>Accuracy not improving</li>\n",
    "  <li>Gradients become 0 (vanish) or NaN (explode)</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Compare ReLU vs Sigmoid:</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c89070",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
